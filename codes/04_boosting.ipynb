{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50d1dc35-8ddb-4f09-9bc0-a7df11313c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_and_import(package, import_name=None):\n",
    "    \"\"\"\n",
    "    Try to import a package. If it's not installed, install it via pip and import again.\n",
    "    package: name on pip (e.g. 'pandas')\n",
    "    import_name: name used in import (e.g. 'pandas' or 'matplotlib.pyplot').\n",
    "                 If None, uses package.\n",
    "    \"\"\"\n",
    "    if import_name is None:\n",
    "        import_name = package\n",
    "\n",
    "    try:\n",
    "        return importlib.import_module(import_name)\n",
    "    except ImportError:\n",
    "        print(f\"{import_name} not found, installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        return importlib.import_module(import_name)\n",
    "\n",
    "# use it for your libs\n",
    "pd = install_and_import(\"pandas\")\n",
    "np = install_and_import(\"numpy\")\n",
    "plt = install_and_import(\"matplotlib.pyplot\", \"matplotlib.pyplot\")\n",
    "# Use the function to install xgboost instead of direct pip command\n",
    "xgb = install_and_import(\"xgboost\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "483b1ddd-20ef-484a-88ee-a5520c6b031f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   site_id_(telephely)  class_size  teachers_number_site  math_score_8_std  \\\n",
      "0              2829211          18                 28.25         -1.675153   \n",
      "1              3353709           9                 13.00          0.265037   \n",
      "2              3359201          13                 13.00         -1.494051   \n",
      "3              3511901          23                 37.00          0.166714   \n",
      "4              3108801          27                 37.50          1.045846   \n",
      "\n",
      "   total_students_site mother_education_level father_education_level  \\\n",
      "0                  266         primary_school         primary_school   \n",
      "1                   82                college  apprenticeship_school   \n",
      "2                  108         primary_school         primary_school   \n",
      "3                  343                college  secondary_with_matura   \n",
      "4                  393  secondary_with_matura  apprenticeship_school   \n",
      "\n",
      "  books_at_home  family_background_index_std student_gender  \\\n",
      "0          0_50                    -2.160581            boy   \n",
      "1     up_to_150                     0.166366           girl   \n",
      "2          0_50                    -1.856121            boy   \n",
      "3          0_50                    -0.007612            boy   \n",
      "4     up_to_300                     0.122871            boy   \n",
      "\n",
      "  class_curriculum_type multiplied_disadvantaged  county_code_school  \\\n",
      "0                normal                      yes                   4   \n",
      "1                normal                       no                  15   \n",
      "2                normal                      yes                  15   \n",
      "3           specialized                       no                   1   \n",
      "4                normal                       no                   9   \n",
      "\n",
      "  settlement_type_school maintainer_type_group_school  \n",
      "0                   town     central_government_state  \n",
      "1      village_1000_2000     central_government_state  \n",
      "2      village_1000_2000     central_government_state  \n",
      "3               Budapest     central_government_state  \n",
      "4            county_seat     central_government_state  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filtered_df_anal = pd.read_csv(r\"C:\\Users\\User\\Downloads\\filtered_data_anal.csv\")\n",
    "\n",
    "# első pár sor ellenőrzéshez:\n",
    "print(filtered_df_anal.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b938cef-a4b7-4379-a7c9-fdbf6754f1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 150 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n150 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\data.py\", line 407, in pandas_feature_info\n    new_feature_types.append(_pandas_dtype_mapper[dtype.name])\n                             ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 'object'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\core.py\", line 774, in inner_f\n    return func(**kwargs)\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1343, in fit\n    train_dmatrix, evals = _wrap_evaluation_matrices(\n                           ~~~~~~~~~~~~~~~~~~~~~~~~~^\n        missing=self.missing,\n        ^^^^^^^^^^^^^^^^^^^^^\n    ...<14 lines>...\n        feature_types=feature_types,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py\", line 702, in _wrap_evaluation_matrices\n    train_dmatrix = create_dmatrix(\n        data=X,\n    ...<9 lines>...\n        ref=None,\n    )\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1257, in _create_dmatrix\n    return QuantileDMatrix(\n        **kwargs, ref=ref, nthread=self.n_jobs, max_bin=self.max_bin\n    )\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\core.py\", line 774, in inner_f\n    return func(**kwargs)\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\core.py\", line 1768, in __init__\n    self._init(\n    ~~~~~~~~~~^\n        data,\n        ^^^^^\n    ...<12 lines>...\n        max_quantile_blocks=max_quantile_batches,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\core.py\", line 1832, in _init\n    it.reraise()\n    ~~~~~~~~~~^^\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\core.py\", line 617, in reraise\n    raise exc  # pylint: disable=raising-bad-type\n    ^^^^^^^^^\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\core.py\", line 598, in _handle_exception\n    return fn()\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\core.py\", line 685, in <lambda>\n    return self._handle_exception(lambda: int(self.next(input_data)), 0)\n                                              ~~~~~~~~~^^^^^^^^^^^^\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\data.py\", line 1632, in next\n    input_data(**self.kwargs)\n    ~~~~~~~~~~^^^^^^^^^^^^^^^\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\core.py\", line 774, in inner_f\n    return func(**kwargs)\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\core.py\", line 665, in input_data\n    new, feature_names, feature_types = _proxy_transform(\n                                        ~~~~~~~~~~~~~~~~^\n        data,\n        ^^^^^\n    ...<2 lines>...\n        self._enable_categorical,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\data.py\", line 1685, in _proxy_transform\n    df, feature_names, feature_types = _transform_pandas_df(\n                                       ~~~~~~~~~~~~~~~~~~~~^\n        data, enable_categorical, feature_names, feature_types\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\data.py\", line 662, in _transform_pandas_df\n    feature_names, feature_types = pandas_feature_info(\n                                   ~~~~~~~~~~~~~~~~~~~^\n        data, meta, feature_names, feature_types, enable_categorical\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\data.py\", line 409, in pandas_feature_info\n    _invalid_dataframe_dtype(data)\n    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\data.py\", line 372, in _invalid_dataframe_dtype\n    raise ValueError(msg)\nValueError: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:mother_education_level: object, father_education_level: object, books_at_home: object, student_gender: object, class_curriculum_type: object, multiplied_disadvantaged: object, settlement_type_school: object, maintainer_type_group_school: object\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 49\u001b[0m\n\u001b[0;32m     37\u001b[0m random_search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[0;32m     38\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mbase_model,\n\u001b[0;32m     39\u001b[0m     param_distributions\u001b[38;5;241m=\u001b[39mparam_distributions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[0;32m     46\u001b[0m )\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Fit the hyperparameter search on the training data\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m random_search\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Best model after hyperparameter tuning\u001b[39;00m\n\u001b[0;32m     52\u001b[0m model \u001b[38;5;241m=\u001b[39m random_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1019\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1020\u001b[0m     )\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1024\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1951\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1951\u001b[0m     evaluate_candidates(\n\u001b[0;32m   1952\u001b[0m         ParameterSampler(\n\u001b[0;32m   1953\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_distributions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state\n\u001b[0;32m   1954\u001b[0m         )\n\u001b[0;32m   1955\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1001\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    995\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    996\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    997\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    998\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    999\u001b[0m     )\n\u001b[1;32m-> 1001\u001b[0m _warn_or_raise_about_fit_failures(out, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_score)\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m   1005\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:517\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    511\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    512\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    515\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    516\u001b[0m     )\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    520\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    521\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    522\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    526\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    527\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 150 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n150 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\data.py\", line 407, in pandas_feature_info\n    new_feature_types.append(_pandas_dtype_mapper[dtype.name])\n                             ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\nKeyError: 'object'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\core.py\", line 774, in inner_f\n    return func(**kwargs)\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1343, in fit\n    train_dmatrix, evals = _wrap_evaluation_matrices(\n                           ~~~~~~~~~~~~~~~~~~~~~~~~~^\n        missing=self.missing,\n        ^^^^^^^^^^^^^^^^^^^^^\n    ...<14 lines>...\n        feature_types=feature_types,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py\", line 702, in _wrap_evaluation_matrices\n    train_dmatrix = create_dmatrix(\n        data=X,\n    ...<9 lines>...\n        ref=None,\n    )\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py\", line 1257, in _create_dmatrix\n    return QuantileDMatrix(\n        **kwargs, ref=ref, nthread=self.n_jobs, max_bin=self.max_bin\n    )\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\core.py\", line 774, in inner_f\n    return func(**kwargs)\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\core.py\", line 1768, in __init__\n    self._init(\n    ~~~~~~~~~~^\n        data,\n        ^^^^^\n    ...<12 lines>...\n        max_quantile_blocks=max_quantile_batches,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\core.py\", line 1832, in _init\n    it.reraise()\n    ~~~~~~~~~~^^\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\core.py\", line 617, in reraise\n    raise exc  # pylint: disable=raising-bad-type\n    ^^^^^^^^^\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\core.py\", line 598, in _handle_exception\n    return fn()\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\core.py\", line 685, in <lambda>\n    return self._handle_exception(lambda: int(self.next(input_data)), 0)\n                                              ~~~~~~~~~^^^^^^^^^^^^\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\data.py\", line 1632, in next\n    input_data(**self.kwargs)\n    ~~~~~~~~~~^^^^^^^^^^^^^^^\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\core.py\", line 774, in inner_f\n    return func(**kwargs)\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\core.py\", line 665, in input_data\n    new, feature_names, feature_types = _proxy_transform(\n                                        ~~~~~~~~~~~~~~~~^\n        data,\n        ^^^^^\n    ...<2 lines>...\n        self._enable_categorical,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\data.py\", line 1685, in _proxy_transform\n    df, feature_names, feature_types = _transform_pandas_df(\n                                       ~~~~~~~~~~~~~~~~~~~~^\n        data, enable_categorical, feature_names, feature_types\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\data.py\", line 662, in _transform_pandas_df\n    feature_names, feature_types = pandas_feature_info(\n                                   ~~~~~~~~~~~~~~~~~~~^\n        data, meta, feature_names, feature_types, enable_categorical\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\data.py\", line 409, in pandas_feature_info\n    _invalid_dataframe_dtype(data)\n    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^\n  File \"C:\\Users\\User\\anaconda3\\Lib\\site-packages\\xgboost\\data.py\", line 372, in _invalid_dataframe_dtype\n    raise ValueError(msg)\nValueError: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:mother_education_level: object, father_education_level: object, books_at_home: object, student_gender: object, class_curriculum_type: object, multiplied_disadvantaged: object, settlement_type_school: object, maintainer_type_group_school: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd                         # For data manipulation and handling DataFrames\n",
    "from xgboost import XGBRegressor            # XGBoost regression model\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV  # Train-test split + hyperparameter search\n",
    "from sklearn.metrics import mean_squared_error, r2_score  # Regression evaluation metrics\n",
    "import numpy as np                          # For numerical operations (e.g., square root)\n",
    "\n",
    "target = \"math_score_8_std\"  # Name of the target variable we want to predict\n",
    "\n",
    "# Feature matrix: drop target and ID columns\n",
    "X = filtered_df_anal.drop(columns=[target])  # Feature matrix\n",
    "y = filtered_df_anal[target]                           # Target vector: the column we want to predict\n",
    "\n",
    "# Train–test split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Base XGBoost model (without fixed hyperparameters)\n",
    "base_model = XGBRegressor(\n",
    "    objective=\"reg:squarederror\",  # explicit regression objective\n",
    "    tree_method=\"hist\",            # efficient histogram-based algorithm\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Hyperparameter search space (coarse but reasonable for an assignment)\n",
    "param_distributions = {\n",
    "    \"n_estimators\": [100, 200, 300, 500, 700],\n",
    "    \"learning_rate\": [0.01, 0.03, 0.05, 0.1, 0.15, 0.2],\n",
    "    \"max_depth\": [3, 4, 5, 6, 7, 8, 10],\n",
    "    \"subsample\": [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "    \"reg_lambda\": [0, 1, 2, 5, 10],   # L2 regularization\n",
    "    \"reg_alpha\": [0, 0.1, 0.25, 0.5, 0.75, 1], # L1 regularization\n",
    "}\n",
    "\n",
    "# Randomized hyperparameter search with cross-validation\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=base_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=30,                      # number of sampled hyperparameter combinations\n",
    "    scoring=\"neg_root_mean_squared_error\",  # we minimize RMSE\n",
    "    cv=5,                           # 5-fold CV\n",
    "    verbose=1,\n",
    "    n_jobs=-1,                      # use all available cores\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Fit the hyperparameter search on the training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model after hyperparameter tuning\n",
    "model = random_search.best_estimator_\n",
    "\n",
    "print(\"Best hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Use the tuned model to predict target values for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Compute evaluation metrics for regression\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R^2:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31a2f7c3-836a-4f64-828f-b4fe970f4077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Best hyperparameters: {'subsample': 0.8, 'reg_lambda': 1, 'reg_alpha': 0.5, 'n_estimators': 700, 'max_depth': 7, 'learning_rate': 0.03, 'colsample_bytree': 1.0}\n",
      "RMSE: 0.8017034061515965\n",
      "R^2: 0.31020580777827944\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd                         # Data manipulation\n",
    "import numpy as np                          # Numerical operations\n",
    "from xgboost import XGBRegressor            # XGBoost regression model\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Define target and features\n",
    "# -----------------------------\n",
    "target = \"math_score_8_std\"                 # Target variable to predict\n",
    "\n",
    "# ID-like columns that should not be used as features (drop only if they exist)\n",
    "id_cols = [\"row_number\", \"student_id\", \"student_serial_number\"]\n",
    "\n",
    "# Build list of columns to drop safely (only those that exist in the DataFrame)\n",
    "cols_to_drop = [target] + id_cols\n",
    "cols_to_drop = [c for c in cols_to_drop if c in filtered_df_anal.columns]\n",
    "\n",
    "# Feature matrix X and target vector y\n",
    "X = filtered_df_anal.drop(columns=cols_to_drop)   # All predictors\n",
    "y = filtered_df_anal[target]                      # Outcome\n",
    "\n",
    "# ------------------------------------\n",
    "# 2. Handle categorical (object) data\n",
    "# ------------------------------------\n",
    "# XGBoost needs numeric or pandas 'category' dtypes\n",
    "cat_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "X[cat_cols] = X[cat_cols].astype(\"category\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Train–test split (80–20)\n",
    "# -----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Base XGBoost model\n",
    "# -----------------------------\n",
    "base_model = XGBRegressor(\n",
    "    objective=\"reg:squarederror\",  # Explicit regression objective\n",
    "    tree_method=\"hist\",            # Required for categorical support / efficiency\n",
    "    enable_categorical=True,       # Tell XGBoost that we use categorical features\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# --------------------------------------------\n",
    "# 5. Hyperparameter search space (Randomized)\n",
    "# --------------------------------------------\n",
    "param_distributions = {\n",
    "    \"n_estimators\": [100, 200, 300, 500, 700],\n",
    "    \"learning_rate\": [0.01, 0.03, 0.05, 0.1, 0.15, 0.2],\n",
    "    \"max_depth\": [3, 4, 5, 6, 7, 8, 10],\n",
    "    \"subsample\": [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "    \"reg_lambda\": [0, 1, 2, 5, 10],             # L2 regularization\n",
    "    \"reg_alpha\": [0, 0.1, 0.25, 0.5, 0.75, 1],  # L1 regularization\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=base_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=30,                         # Number of sampled hyperparameter combinations\n",
    "    scoring=\"neg_root_mean_squared_error\",  # Minimize RMSE\n",
    "    cv=5,                              # 5-fold cross-validation\n",
    "    verbose=1,\n",
    "    n_jobs=-1,                         # Use all available cores\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 6. Fit hyperparameter search & evaluate model\n",
    "# ---------------------------------------------\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model after tuning\n",
    "model = random_search.best_estimator_\n",
    "\n",
    "print(\"Best hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Predictions on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Regression metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R^2:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d634223-57df-48e3-9048-2be4f28bca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "from xgboost import plot_importance\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Global plotting style for publication-quality figures\n",
    "# -------------------------------------------------------------------\n",
    "mpl.rcParams.update({\n",
    "    \"font.family\": \"Times New Roman\",\n",
    "    \"font.size\": 11,\n",
    "    \"axes.titlesize\": 13,\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"xtick.labelsize\": 10,\n",
    "    \"ytick.labelsize\": 10,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.spines.right\": False,\n",
    "    \"figure.dpi\": 600,\n",
    "})\n",
    "\n",
    "save_dir = r\"C:\\Users\\User\\Desktop\\Figures\\Figures\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. True vs. predicted values\n",
    "# -------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "ax.scatter(y_test, y_pred, s=20, alpha=0.6)\n",
    "ax.set_xlabel(\"True $math\\\\_score\\\\_8\\\\_std$\")\n",
    "ax.set_ylabel(\"Predicted $math\\\\_score\\\\_8\\\\_std$\")\n",
    "ax.set_title(\"True vs. predicted values\")\n",
    "\n",
    "min_val = min(y_test.min(), y_pred.min())\n",
    "max_val = max(y_test.max(), y_pred.max())\n",
    "ax.plot([min_val, max_val], [min_val, max_val],\n",
    "        linestyle=\"--\", linewidth=1)\n",
    "\n",
    "ax.set_xlim(min_val, max_val)\n",
    "ax.set_ylim(min_val, max_val)\n",
    "\n",
    "ax.grid(True, linestyle=\":\", linewidth=0.5, alpha=0.7)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(save_dir, \"true_vs_predicted.png\"), dpi=600)\n",
    "plt.close(fig)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. Residuals histogram\n",
    "# -------------------------------------------------------------------\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "ax.hist(residuals, bins=30, edgecolor=\"black\", alpha=0.8)\n",
    "ax.set_xlabel(\"Residual (true – predicted)\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_title(\"Distribution of residuals\")\n",
    "\n",
    "ax.axvline(0, linestyle=\"--\", linewidth=1)\n",
    "\n",
    "ax.grid(True, linestyle=\":\", linewidth=0.5, alpha=0.7)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(save_dir, \"residuals_histogram.png\"), dpi=600)\n",
    "plt.close(fig)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. Residuals vs. fitted values\n",
    "# -------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "ax.scatter(y_pred, residuals, s=20, alpha=0.6)\n",
    "ax.axhline(0, linestyle=\"--\", linewidth=1)\n",
    "\n",
    "ax.set_xlabel(\"Predicted $math\\\\_score\\\\_8\\\\_std$\")\n",
    "ax.set_ylabel(\"Residual (true – predicted)\")\n",
    "ax.set_title(\"Residuals vs. predicted values\")\n",
    "\n",
    "ax.grid(True, linestyle=\":\", linewidth=0.5, alpha=0.7)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(save_dir, \"residuals_vs_fitted.png\"), dpi=600)\n",
    "plt.close(fig)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. Feature importance (XGBoost)\n",
    "# -------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(figsize=(7, 9))\n",
    "\n",
    "plot_importance(\n",
    "    model,\n",
    "    max_num_features=20,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_title(\"Top 20 feature importances (XGBoost)\")\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(save_dir, \"feature_importance.png\"), dpi=600)\n",
    "plt.close(fig)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
